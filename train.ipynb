{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset.csv').to_numpy()\n",
    "sounds_dataset = dataset_df[:, 0]\n",
    "sr = 0\n",
    "for i in range(len(sounds_dataset)):\n",
    "    sounds_dataset[i], sr = librosa.load(f'Noisy-sounds/{sounds_dataset[i]}.wav')\n",
    "\n",
    "label_dataset = dataset_df[:, 1]\n",
    "for i in range(len(label_dataset)):\n",
    "    label_dataset[i], sr = librosa.load(f'Clean-sounds/{label_dataset[i]}.wav')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sounds_dataset, label_dataset, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for x in X_train:\n",
    "    if len(x) > max_len:\n",
    "        max_len = len(x)\n",
    "\n",
    "for x in X_test:\n",
    "    if len(x) > max_len:\n",
    "        max_len = len(x)\n",
    "\n",
    "for x in y_train:\n",
    "    if len(x) > max_len:\n",
    "        max_len = len(x)\n",
    "\n",
    "for x in y_test:\n",
    "    if len(x) > max_len:\n",
    "        max_len = len(x)\n",
    "\n",
    "\n",
    "def pad_vector(vec):\n",
    "    for i in range(len(vec)):\n",
    "        vec[i] = np.pad(vec[i], (0, max_len - len(vec[i])))\n",
    "        # vec[i] = np.asarray(vec[i]).astype(np.float32)\n",
    "    return vec\n",
    "\n",
    "\n",
    "X_train = pad_vector(X_train)\n",
    "X_test = pad_vector(X_test)\n",
    "y_train = pad_vector(y_train)\n",
    "y_test = pad_vector(y_test)\n",
    "\n",
    "X_train_good = np.ones((len(X_train), max_len))\n",
    "X_test_good = np.ones((len(X_test), max_len))\n",
    "y_train_good = np.ones((len(y_train), max_len))\n",
    "y_test_good = np.ones((len(y_test), max_len))\n",
    "\n",
    "for i in range(len(X_train_good)):\n",
    "    X_train_good[i] = X_train[i]\n",
    "for i in range(len(X_test_good)):\n",
    "    X_test_good[i] = X_test[i]\n",
    "for i in range(len(y_train_good)):\n",
    "    y_train_good[i] = y_train[i]\n",
    "for i in range(len(y_test_good)):\n",
    "    y_test_good[i] = y_test[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3.6621094e-04, 4.5776367e-04, 9.1552734e-05, ..., 0.0000000e+00,\n",
      "        0.0000000e+00, 0.0000000e+00], dtype=float32)\n",
      " array([0.00134277, 0.00146484, 0.00216675, ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32)\n",
      " array([0.00268555, 0.00326538, 0.003479  , ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32)\n",
      " ... array([-0.00457764, -0.00479126, -0.00375366, ...,  0.        ,\n",
      "             0.        ,  0.        ], dtype=float32)\n",
      " array([-0.01177979, -0.0133667 , -0.01184082, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32)\n",
      " array([0.00762939, 0.00839233, 0.00579834, ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32)                                     ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = max_len  # Dimensionality of input data (e.g., length of audio signal)\n",
    "latent_dim = 128  # Dimensionality of the latent space\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the model\n",
    "model = DenoisingAutoencoder(latent_dim)\n",
    "\n",
    "# Convert to CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178/178 [==============================] - 74s 400ms/step - loss: 0.0439 - val_loss: 0.0017\n",
      "Epoch 2/5\n",
      "178/178 [==============================] - 63s 353ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 3/5\n",
      "178/178 [==============================] - 62s 346ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 4/5\n",
      "178/178 [==============================] - 61s 345ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 5/5\n",
      "178/178 [==============================] - 61s 344ms/step - loss: 0.0018 - val_loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Define the model\n",
    "autoencoder = DenoisingAutoencoder(latent_dim)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')  # Using Mean Squared Error loss for audio reconstruction\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(X_train_good, y_train_good, epochs=epochs, batch_size=batch_size, validation_data=(X_test_good, y_test_good))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "class DatasetLoader(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(X_train_good)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index 0 ar fi header ul\n",
    "        # signal, sr = sf.read(self.df['ActorID'].to_numpy()[index])  ### Load the waveform using librosa and sample rate = 16000\n",
    "        # label = (self.df['Gender'].to_numpy()[index], self.df['Age'].to_numpy()[index]) ### Load the label from DataFrame\n",
    "\n",
    "        # stft = librosa.stft(signal)\n",
    "        # stft = np.abs(stft) ** 2\n",
    "        # stft = stft / np.max((np.abs(stft)))\n",
    "        # stft = 20 * np.log10(stft)\n",
    "        # stft = np.clip(stft, -80, 0)\n",
    "\n",
    "        # stft = np.expand_dims(stft, axis=0).astype(np.float32)\n",
    "        ### STFT for the signal\n",
    "        ### normlize, clip (-80, 0).\n",
    "        ### STFT shape !\n",
    "        return X_train_good[index].astype(np.float32), y_train_good[index].astype(np.float32)\n",
    "\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 15\n",
    "    LR = 1e-4\n",
    "    BAR_FORMAT = '{l_bar}{bar:10}{r_bar}{bar:-10b}'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"model running on device: {DEVICE}\\n\")\n",
    "    train = DatasetLoader()\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = DenoisingAutoencoder(latent_dim)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)  ### Optimizer\n",
    "\n",
    "    error = 0\n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"+++++++++++++RUNNING EPOCH: {i} ++++++++++++++++\")\n",
    "\n",
    "        ct = 0\n",
    "        print(\"=============TRAINING===============\")\n",
    "\n",
    "        model.train().to(device=DEVICE)\n",
    "        for x, y in tqdm(train_loader, bar_format=BAR_FORMAT):\n",
    "            pred_sound = model(x)\n",
    "            loss = loss_fn(pred_sound, y.float())\n",
    "            print(loss, pred_sound, y.float(), torch.max(y.float()), torch.max(pred_sound))\n",
    "            break\n",
    "            error += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ct += 1\n",
    "\n",
    "        error /= ct\n",
    "        print(f\"Error: {error}\")\n",
    "        print(\" \")\n",
    "\n",
    "        # print(\"=============TESTING===============\")\n",
    "\n",
    "        # preds = np.array([])\n",
    "        # labels = np.array([])\n",
    "        # mae = 0\n",
    "        # model.eval().to(device=DEVICE)\n",
    "        # for x, y in tqdm(test_loader, bar_format=BAR_FORMAT):\n",
    "        #     with torch.no_grad():\n",
    "        #         pred_gender, pred_age = model(x)\n",
    "        #         pred_age = torch.squeeze(pred_age)\n",
    "\n",
    "        #         predicted = torch.argmax(pred_gender, -1)\n",
    "        #         predicted = predicted.detach().cpu()\n",
    "        #         y[0] = y[0].detach().cpu().numpy()\n",
    "        #         mae = (pred_age - y[1].float()).abs().mean()\n",
    "\n",
    "        #         preds = np.concatenate((preds, predicted), axis=0)\n",
    "        #         labels = np.concatenate((labels, y[0]), axis=0)\n",
    "\n",
    "        # acc = (preds == labels).sum() / len(labels)\n",
    "        # f1 = metrics.f1_score(preds, labels)\n",
    "        # print(f\"mae: {mae:.3f}\")\n",
    "        # print(f\"accuracy test: {acc:.3f}\")\n",
    "        # print(f\"f1 score test: {f1}\")\n",
    "        # print(\" \")\n",
    "\n",
    "        # if acc > optim:\n",
    "        #     optim = acc\n",
    "        #     saved_epoch = i\n",
    "        #     print(f\"model with {optim} accuracy saved at epoch {i}\")\n",
    "        #     torch.save(model.state_dict(), \"saved_model.pt\")\n",
    "\n",
    "    #print(f\"model with {optim} accuracy saved at epoch {saved_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running on device: cpu\n",
      "\n",
      "+++++++++++++RUNNING EPOCH: 0 ++++++++++++++++\n",
      "=============TRAINING===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0028, grad_fn=<MseLossBackward0>) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0408, 0.0182],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0426, 0.0178],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0401, 0.0179],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0393, 0.0202],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0425, 0.0184],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0401, 0.0190]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[-0.0079, -0.0088, -0.0077,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0030,  0.0031,  0.0017,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0021, -0.0028, -0.0027,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0063,  0.0055,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0077,  0.0088,  0.0073,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0021, -0.0011, -0.0005,  ...,  0.0000,  0.0000,  0.0000]]) tensor(1.0351) tensor(0.1583, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m     ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 65\u001b[0m error \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m ct\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 110361)\n",
      "1/1 [==============================] - 0s 85ms/step\n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'test_denoised.wav': Format not recognised.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m prediciton \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mpredict(test)\n\u001b[1;32m----> 5\u001b[0m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_denoised.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediciton\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCM_24\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\soundfile.py:343\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, samplerate, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     channels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m               \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    345\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'test_denoised.wav': Format not recognised."
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "test = np.expand_dims(test, axis=0)\n",
    "print(test.shape)\n",
    "prediciton = autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 110361)\n"
     ]
    }
   ],
   "source": [
    "print(prediciton.shape)\n",
    "sf.write('test_denoised.wav', prediciton[0], sr, subtype='PCM_24')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
