{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 16:35:34.346979: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-16 16:35:34.369419: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 16:35:34.369431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 16:35:34.369947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-16 16:35:34.373711: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 16:35:34.744243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset.csv').to_numpy()\n",
    "sounds_dataset = dataset_df[:, 0]\n",
    "sr = 0\n",
    "for i in range(len(sounds_dataset)):\n",
    "    sounds_dataset[i], sr = librosa.load(f'Noisy-sounds/{sounds_dataset[i]}.wav')\n",
    "\n",
    "label_dataset = dataset_df[:, 1]\n",
    "for i in range(len(label_dataset)):\n",
    "    label_dataset[i], sr = librosa.load(f'Clean-sounds/{label_dataset[i]}.wav')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sounds_dataset, label_dataset, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spectogram(sound):\n",
    "    stft = librosa.stft(sound.astype(np.float64), n_fft = 512, hop_length=256)\n",
    "    stft = np.abs(stft)\n",
    "    db = librosa.amplitude_to_db(stft, ref=np.max)\n",
    "    return db\n",
    "    # print(db)\n",
    "    # librosa.display.specshow(db, y_axis='log', x_axis='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram_train = []\n",
    "for x in X_train:\n",
    "    spectogram_train.append(to_spectogram(x))\n",
    "spectogram_label = []\n",
    "for x in y_train:\n",
    "    spectogram_label.append(to_spectogram(x))\n",
    "spectogram_val = []\n",
    "for x in X_test:\n",
    "    spectogram_val.append(to_spectogram(x))\n",
    "spectogram_val_label = []\n",
    "for x in y_test:\n",
    "    spectogram_val_label.append(to_spectogram(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for x in spectogram_train:\n",
    "    max_len = max(max_len, len(x[0]))\n",
    "for x in spectogram_label:\n",
    "    max_len = max(max_len, len(x[0]))\n",
    "for x in spectogram_val:\n",
    "    max_len = max(max_len, len(x[0]))\n",
    "for x in spectogram_val_label:\n",
    "    max_len = max(max_len, len(x[0]))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vector(vec):\n",
    "    for i in range(len(vec)):\n",
    "        vec[i] = np.pad(vec[i], ((0, 0), (0, max_len - len(vec[i][0]))))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram_train = pad_vector(spectogram_train)\n",
    "spectogram_label = pad_vector(spectogram_label)\n",
    "spectogram_val = pad_vector(spectogram_val)\n",
    "spectogram_val_label = pad_vector(spectogram_val_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram_train = np.array(spectogram_train)\n",
    "spectogram_label = np.array(spectogram_label)\n",
    "spectogram_val = np.array(spectogram_val)\n",
    "spectogram_val_label = np.array(spectogram_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5675, 257, 432)\n"
     ]
    }
   ],
   "source": [
    "print(spectogram_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-45.30446193, -42.96131058, -32.1388143 , ..., -39.13113703,\n",
       "        -50.66475932, -55.05316011],\n",
       "       [-50.76353458, -35.92381895, -27.72237326, ..., -38.13283152,\n",
       "        -31.65367052, -37.44977029],\n",
       "       [-38.69418165, -34.68762398, -25.89603874, ..., -38.03767509,\n",
       "        -29.08121816, -32.82540258],\n",
       "       ...,\n",
       "       [-80.        , -80.        , -80.        , ..., -80.        ,\n",
       "        -80.        , -80.        ],\n",
       "       [-80.        , -80.        , -80.        , ..., -80.        ,\n",
       "        -80.        , -80.        ],\n",
       "       [-80.        , -80.        , -80.        , ..., -80.        ,\n",
       "        -80.        , -80.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_spectogram(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 54, 128), (None, 16, 27, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m up6 \u001b[38;5;241m=\u001b[39m Conv2D(size_filter_in\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m, activation \u001b[38;5;241m=\u001b[39m activation_layer, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer \u001b[38;5;241m=\u001b[39m kernel_init)(drop5)\n\u001b[1;32m     44\u001b[0m up6 \u001b[38;5;241m=\u001b[39m LeakyReLU()(up6)\n\u001b[0;32m---> 45\u001b[0m merge6 \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdrop4\u001b[49m\u001b[43m,\u001b[49m\u001b[43mup6\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m conv6 \u001b[38;5;241m=\u001b[39m Conv2D(size_filter_in\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation \u001b[38;5;241m=\u001b[39m activation_layer, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer \u001b[38;5;241m=\u001b[39m kernel_init)(merge6)\n\u001b[1;32m     47\u001b[0m conv6 \u001b[38;5;241m=\u001b[39m LeakyReLU()(conv6)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/merging/concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    125\u001b[0m unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    126\u001b[0m     shape[axis]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 54, 128), (None, 16, 27, 128)]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D, BatchNormalization, LeakyReLU, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "# Define the dimensions of the input data\n",
    "input_shape = (257, 432, 1)  # MNIST dataset has 28x28 grayscale images\n",
    "\n",
    "# Define the input layer\n",
    "input_img = Input(shape=input_shape)\n",
    "size_filter_in = 16\n",
    "kernel_init = 'he_normal'\n",
    "\n",
    "activation_layer = None\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(inputs)\n",
    "conv1 = LeakyReLU()(conv1)\n",
    "conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv1)\n",
    "conv1 = LeakyReLU()(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool1)\n",
    "conv2 = LeakyReLU()(conv2)\n",
    "conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv2)\n",
    "conv2 = LeakyReLU()(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool2)\n",
    "conv3 = LeakyReLU()(conv3)\n",
    "conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv3)\n",
    "conv3 = LeakyReLU()(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool3)\n",
    "conv4 = LeakyReLU()(conv4)\n",
    "conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv4)\n",
    "conv4 = LeakyReLU()(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool4)\n",
    "conv5 = LeakyReLU()(conv5)\n",
    "conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv5)\n",
    "conv5 = LeakyReLU()(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(drop5)\n",
    "up6 = LeakyReLU()(up6)\n",
    "merge6 = concatenate([drop4,up6], axis = 3)\n",
    "conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge6)\n",
    "conv6 = LeakyReLU()(conv6)\n",
    "conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
    "conv6 = LeakyReLU()(conv6)\n",
    "up7 = Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
    "up7 = LeakyReLU()(up7)\n",
    "merge7 = concatenate([conv3,up7], axis = 3)\n",
    "conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge7)\n",
    "conv7 = LeakyReLU()(conv7)\n",
    "conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
    "conv7 = LeakyReLU()(conv7)\n",
    "up8 = Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
    "up8 = LeakyReLU()(up8)\n",
    "merge8 = concatenate([conv2,up8], axis = 3)\n",
    "conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge8)\n",
    "conv8 = LeakyReLU()(conv8)\n",
    "conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
    "conv8 = LeakyReLU()(conv8)\n",
    "\n",
    "up9 = Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
    "up9 = LeakyReLU()(up9)\n",
    "merge9 = concatenate([conv1,up9], axis = 3)\n",
    "conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge9)\n",
    "conv9 = LeakyReLU()(conv9)\n",
    "conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "conv9 = LeakyReLU()(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "conv9 = LeakyReLU()(conv9)\n",
    "conv10 = Conv2D(1, 1, activation = 'tanh')(conv9)\n",
    "\n",
    "model = Model(inputs,conv10)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "#model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(spectogram_train, spectogram_label,\n",
    "                epochs=5,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(spectogram_val, spectogram_val_label),)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
